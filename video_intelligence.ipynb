{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8aad1d-a502-4cff-abf0-fcf9d982672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import ffmpeg\n",
    "import moviepy.editor as mp\n",
    "import cv2\n",
    "import io\n",
    "import PIL\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "import pickle\n",
    "import base64\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import vision\n",
    "from PIL import Image\n",
    "\n",
    "os.chdir(\"/nfs/sloanlab003/projects/arc_proj/story/data/sb_ad_links\")\n",
    "LINK_DIR = \"/nfs/sloanlab003/projects/arc_proj/story/data/sb_ad_links\"\n",
    "PARENT_DIR = \"/nfs/sloanlab003/projects/arc_proj/story/data/sb_ad_vids\"\n",
    "df = pd.read_csv(\"nw_may_v1.csv\")\n",
    "\n",
    "from google.cloud import videointelligence_v1 as videointelligence\n",
    "from google.cloud import vision\n",
    "\n",
    "os.chdir(\"/nfs/sloanlab003/projects/arc_proj/story/code/nw/google-cloud-sdk\")\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"/nfs/sloanlab003/projects/arc_proj/story/code/nw/ad-story-arc-b60c485322b4.json\"\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca39e84-d5d3-4a10-8ba0-7c57a8c60f38",
   "metadata": {},
   "source": [
    "## Google Video Intelligence (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa2b6b-6241-4afe-814d-1c4463b1aaab",
   "metadata": {},
   "source": [
    "### Label detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09305e7b-f26d-46a6-a6ca-bd25c2965237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_labels(path):\n",
    "    from google.oauth2 import service_account\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    \"\"\"Detect labels given a file path.\"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\"/nfs/sloanlab003/projects/arc_proj/story/code/nw/ad-story-arc-b60c485322b4.json\")\n",
    "    video_client = videointelligence.VideoIntelligenceServiceClient(credentials=credentials)\n",
    "    features = [videointelligence.Feature.LABEL_DETECTION]\n",
    "\n",
    "\n",
    "    with io.open(path, \"rb\") as movie:\n",
    "        input_content = movie.read()\n",
    "\n",
    "    operation = video_client.annotate_video(\n",
    "        request={\"features\": features, \"input_content\": input_content}\n",
    "    )\n",
    "    print(\"\\nProcessing video for label annotations:\")\n",
    "\n",
    "    result = operation.result(timeout=90)\n",
    "    print(\"\\nFinished processing.\")\n",
    "    \n",
    "\n",
    "    # Process video/segment level label annotations\n",
    "    segment_labels = result.annotation_results[0].segment_label_annotations\n",
    "    for i, segment_label in enumerate(segment_labels):\n",
    "        print(\"Video label description: {}\".format(segment_label.entity.description))\n",
    "        for category_entity in segment_label.category_entities:\n",
    "            print(\n",
    "                \"\\tLabel category description: {}\".format(category_entity.description)\n",
    "            )\n",
    "\n",
    "        for i, segment in enumerate(segment_label.segments):\n",
    "            start_time = (\n",
    "                segment.segment.start_time_offset.seconds\n",
    "                + segment.segment.start_time_offset.microseconds / 1e6\n",
    "            )\n",
    "            end_time = (\n",
    "                segment.segment.end_time_offset.seconds\n",
    "                + segment.segment.end_time_offset.microseconds / 1e6\n",
    "            )\n",
    "            positions = \"{}s to {}s\".format(start_time, end_time)\n",
    "            confidence = segment.confidence\n",
    "            print(\"\\tSegment {}: {}\".format(i, positions))\n",
    "            print(\"\\tConfidence: {}\".format(confidence))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Process shot level label annotations\n",
    "    shot_labels = result.annotation_results[0].shot_label_annotations\n",
    "    for i, shot_label in enumerate(shot_labels):\n",
    "        print(\"Shot label description: {}\".format(shot_label.entity.description))\n",
    "        print(type(shot_label.category_entities), len(shot_label.category_entities))\n",
    "        for category_entity in shot_label.category_entities:\n",
    "            print(\n",
    "                \"\\tLabel category description: {}\".format(category_entity.description)\n",
    "            )\n",
    "\n",
    "        for i, shot in enumerate(shot_label.segments):\n",
    "            start_time = (\n",
    "                shot.segment.start_time_offset.seconds\n",
    "                + shot.segment.start_time_offset.microseconds / 1e6\n",
    "            )\n",
    "            end_time = (\n",
    "                shot.segment.end_time_offset.seconds\n",
    "                + shot.segment.end_time_offset.microseconds / 1e6\n",
    "            )\n",
    "            positions = \"{}s to {}s\".format(start_time, end_time)\n",
    "            confidence = shot.confidence\n",
    "            print(\"\\tSegment {}: {}\".format(i, positions))\n",
    "            print(\"\\tConfidence: {}\".format(confidence))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Process frame level label annotations\n",
    "    frame_labels = result.annotation_results[0].frame_label_annotations\n",
    "    for i, frame_label in enumerate(frame_labels):\n",
    "        print(\"Frame label description: {}\".format(frame_label.entity.description))\n",
    "        for category_entity in frame_label.category_entities:\n",
    "            print(\n",
    "                \"\\tLabel category description: {}\".format(category_entity.description)\n",
    "            )\n",
    "\n",
    "        # Each frame_label_annotation has many frames,\n",
    "        # here we print information only about the first frame.\n",
    "        frame = frame_label.frames[0]\n",
    "        time_offset = frame.time_offset.seconds + frame.time_offset.microseconds / 1e6\n",
    "        print(\"\\tFirst frame time offset: {}s\".format(time_offset))\n",
    "        print(\"\\tFirst frame confidence: {}\".format(frame.confidence))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f03e5-59e4-4d6c-a59f-a7c6e80ae13c",
   "metadata": {},
   "source": [
    "### Logo Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d650c1d-f9b8-4c27-b6f2-77e5fb9cee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_logo(local_file_path):\n",
    "    \"\"\"Performs asynchronous video annotation for logo recognition on a local file.\"\"\"\n",
    "    \n",
    "    credentials = service_account.Credentials.from_service_account_file(\"/nfs/sloanlab003/projects/arc_proj/story/code/nw/ad-story-arc-b60c485322b4.json\")\n",
    "    client = videointelligence.VideoIntelligenceServiceClient(credentials=credentials)\n",
    "\n",
    "    with io.open(local_file_path, \"rb\") as f:\n",
    "        input_content = f.read()\n",
    "    features = [videointelligence.Feature.LOGO_RECOGNITION]\n",
    "\n",
    "    operation = client.annotate_video(\n",
    "        request={\"features\": features, \"input_content\": input_content}\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    response = operation.result()\n",
    "\n",
    "    # Get the first response, since we sent only one video.\n",
    "    annotation_result = response.annotation_results[0]\n",
    "\n",
    "    # Annotations for list of logos detected, tracked and recognized in video.\n",
    "    for logo_recognition_annotation in annotation_result.logo_recognition_annotations:\n",
    "        entity = logo_recognition_annotation.entity\n",
    "\n",
    "        # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\n",
    "        # Search API](https://developers.google.com/knowledge-graph/).\n",
    "        print(\"Entity Id : {}\".format(entity.entity_id))\n",
    "\n",
    "        print(\"Description : {}\".format(entity.description))\n",
    "\n",
    "        # All logo tracks where the recognized logo appears. Each track corresponds\n",
    "        # to one logo instance appearing in consecutive frames.\n",
    "        for track in logo_recognition_annotation.tracks:\n",
    "            # Video segment of a track.\n",
    "            print(\n",
    "                \"\\n\\tStart Time Offset : {}.{}\".format(\n",
    "                    track.segment.start_time_offset.seconds,\n",
    "                    track.segment.start_time_offset.microseconds * 1000,\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                \"\\tEnd Time Offset : {}.{}\".format(\n",
    "                    track.segment.end_time_offset.seconds,\n",
    "                    track.segment.end_time_offset.microseconds * 1000,\n",
    "                )\n",
    "            )\n",
    "            print(\"\\tConfidence : {}\".format(track.confidence))\n",
    "\n",
    "            # The object with timestamp and attributes per frame in the track.\n",
    "            for timestamped_object in track.timestamped_objects:\n",
    "\n",
    "                # Normalized Bounding box in a frame, where the object is located.\n",
    "                normalized_bounding_box = timestamped_object.normalized_bounding_box\n",
    "                # print(\"\\n\\t\\tLeft : {}\".format(normalized_bounding_box.left))\n",
    "                # print(\"\\t\\tTop : {}\".format(normalized_bounding_box.top))\n",
    "                # print(\"\\t\\tRight : {}\".format(normalized_bounding_box.right))\n",
    "                # print(\"\\t\\tBottom : {}\".format(normalized_bounding_box.bottom))\n",
    "\n",
    "                # Optional. The attributes of the object in the bounding box.\n",
    "                for attribute in timestamped_object.attributes:\n",
    "                    print(\"\\n\\t\\t\\tName : {}\".format(attribute.name))\n",
    "                    print(\"\\t\\t\\tConfidence : {}\".format(attribute.confidence))\n",
    "                    print(\"\\t\\t\\tValue : {}\".format(attribute.value))\n",
    "\n",
    "            # Optional. Attributes in the track level.\n",
    "            for track_attribute in track.attributes:\n",
    "                print(\"\\n\\t\\tName : {}\".format(track_attribute.name))\n",
    "                print(\"\\t\\tConfidence : {}\".format(track_attribute.confidence))\n",
    "                print(\"\\t\\tValue : {}\".format(track_attribute.value))\n",
    "\n",
    "        # All video segments where the recognized logo appears. There might be\n",
    "        # multiple instances of the same logo class appearing in one VideoSegment.\n",
    "        for segment in logo_recognition_annotation.segments:\n",
    "            print(\n",
    "                \"\\n\\tStart Time Offset : {}.{}\".format(\n",
    "                    segment.start_time_offset.seconds,\n",
    "                    segment.start_time_offset.microseconds * 1000,\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                \"\\tEnd Time Offset : {}.{}\".format(\n",
    "                    segment.end_time_offset.seconds,\n",
    "                    segment.end_time_offset.microseconds * 1000,\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67843630-4fcf-403e-a136-d90970aec44d",
   "metadata": {},
   "source": [
    "### Text Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58760773-f80d-4871-9ee7-05d58114665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_detect_text(path):\n",
    "\n",
    "    \"\"\"Detect text in a local video.\"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\"/nfs/sloanlab003/projects/arc_proj/story/code/nw/ad-story-arc-b60c485322b4.json\")\n",
    "    video_client = videointelligence.VideoIntelligenceServiceClient(credentials=credentials)\n",
    "    features = [videointelligence.Feature.TEXT_DETECTION]\n",
    "    video_context = videointelligence.VideoContext()\n",
    "\n",
    "    with io.open(path, \"rb\") as file:\n",
    "        input_content = file.read()\n",
    "\n",
    "    operation = video_client.annotate_video(\n",
    "        request={\n",
    "            \"features\": features,\n",
    "            \"input_content\": input_content,\n",
    "            \"video_context\": video_context,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\nProcessing video for text detection.\")\n",
    "    result = operation.result(timeout=300)\n",
    "\n",
    "    # The first result is retrieved because a single video was processed.\n",
    "    annotation_result = result.annotation_results[0]\n",
    "\n",
    "    for text_annotation in annotation_result.text_annotations:\n",
    "        print(\"\\nText: {}\".format(text_annotation.text))\n",
    "\n",
    "        # Get the first text segment\n",
    "        text_segment = text_annotation.segments[0]\n",
    "        start_time = text_segment.segment.start_time_offset\n",
    "        end_time = text_segment.segment.end_time_offset\n",
    "        print(\n",
    "            \"start_time: {}, end_time: {}\".format(\n",
    "                start_time.seconds + start_time.microseconds * 1e-6,\n",
    "                end_time.seconds + end_time.microseconds * 1e-6,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"Confidence: {}\".format(text_segment.confidence))\n",
    "\n",
    "        # Show the result for the first frame in this segment.\n",
    "        frame = text_segment.frames[0]\n",
    "        time_offset = frame.time_offset\n",
    "        print(\n",
    "            \"Time offset for the first frame: {}\".format(\n",
    "                time_offset.seconds + time_offset.microseconds * 1e-6\n",
    "            )\n",
    "        )\n",
    "        print(\"Rotated Bounding Box Vertices:\")\n",
    "        for vertex in frame.rotated_bounding_box.vertices:\n",
    "            print(\"\\tVertex.x: {}, Vertex.y: {}\".format(vertex.x, vertex.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8a053-cf08-4c6b-8a1f-75869c2d2e22",
   "metadata": {},
   "source": [
    "### Save to Pickle file\n",
    "Takes all features listed in features list and runs video intelligence API to obtain said features. Features can be customized. Face detection done seperately (see bottom) due to runtime.\n",
    "\n",
    "All output is saved immediately to pickle file, pickle file saved to correspoinding folder by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558830cf-4363-4be1-843b-a5df28ad5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle(ID):\n",
    "    from google.oauth2 import service_account\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    \n",
    "    os.chdir(f'{PARENT_DIR}/{ID}')\n",
    "    path = ID+\".mp4\"\n",
    "    pickle_name = f'{ID}_video_intelligence.pickle'\n",
    "    pickle_dict = {}\n",
    "    features = [videointelligence.Feature.LABEL_DETECTION, videointelligence.Feature.LOGO_RECOGNITION, videointelligence.Feature.TEXT_DETECTION]\n",
    "\n",
    "    \n",
    "    \"\"\"Detect labels given a file path.\"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\"/nfs/sloanlab003/projects/arc_proj/story/code/nw/ad-story-arc-b60c485322b4.json\")\n",
    "    video_client = videointelligence.VideoIntelligenceServiceClient(credentials=credentials)\n",
    "   \n",
    "    # face detection config - comment out if not using face detection\n",
    "    config = videointelligence.FaceDetectionConfig(\n",
    "        include_bounding_boxes=True, include_attributes=True\n",
    "    )\n",
    "    context = videointelligence.VideoContext(face_detection_config=config)\n",
    "    \n",
    "\n",
    "\n",
    "    with io.open(path, \"rb\") as movie:\n",
    "        input_content = movie.read()\n",
    "\n",
    "    operation = video_client.annotate_video(\n",
    "        request={\"features\": features, \"input_content\": input_content, \"video_context\": context}\n",
    "    )\n",
    "    print(\"\\nProcessing video for logo annotations:\", ID)\n",
    "\n",
    "    result = operation.result(timeout=500)\n",
    "    print(\"\\nFinished processing.\")\n",
    "    pickle_dict[\"logo detection\"] = list(result.annotation_results[0].logo_recognition_annotations)\n",
    "    pickle_dict[\"text detection\"] = list(result.annotation_results[0].text_annotations)\n",
    "    pickle_dict[\"segment labels\"] = list(result.annotation_results[0].segment_label_annotations)\n",
    "    pickle_dict[\"shot labels\"] = list(result.annotation_results[0].shot_label_annotations)\n",
    "    pickle_dict[\"frame labels\"] = list(result.annotation_results[0].frame_label_annotations)\n",
    "    \n",
    "#     # second round\n",
    "\n",
    "    with io.open(path, \"rb\") as f:\n",
    "        input_content = f.read()\n",
    "\n",
    "    # Configure the request\n",
    "    config = videointelligence.FaceDetectionConfig(\n",
    "        include_bounding_boxes=True, include_attributes=True\n",
    "    )\n",
    "    context = videointelligence.VideoContext(face_detection_config=config)\n",
    "\n",
    "    # Start the asynchronous request\n",
    "    operation = video_client.annotate_video(\n",
    "        request={\n",
    "            \"features\": [videointelligence.Feature.FACE_DETECTION],\n",
    "            \"input_content\": input_content,\n",
    "            \"video_context\": context,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\nProcessing video for face detection annotations.\")\n",
    "    result = operation.result(timeout=300)\n",
    "\n",
    "    print(\"\\nFinished processing.\\n\")\n",
    "    \n",
    "    pickle_dict[\"facial detection\"] = list(result.annotation_results[0].face_detection_annotations)\n",
    "\n",
    "\n",
    "    with open(f'{PARENT_DIR}/{ID}/{pickle_name}', \"wb\") as handle:\n",
    "        pickle.dump(pickle_dict, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"done saving pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba590179-2440-4362-a880-95a7e454d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = 'sb_2023_3515'\n",
    "os.chdir(f\"{PARENT_DIR}/{id_test}\")\n",
    "\n",
    "analyze_labels(id_test+'.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff71f70-bd2a-41bc-91dd-90eff9e9dd4d",
   "metadata": {},
   "source": [
    "## Read Pickle\n",
    "\n",
    "Takes pickle file from previous function (should be saved in corresponding folder by ID) and creates seperate CSV files for each feature.\n",
    "\n",
    "Logo detection: {ID}_logo_info.csv\n",
    "\n",
    "Text detection: {ID}_text_info.csv\n",
    "\n",
    "Face detection: {ID}_faces_info.csv\n",
    "\n",
    "Label detection (saves 3 seperate CSVs): {ID}_labelvideo_info.csv, {ID}_labelshot_info.csv, {ID}_labelframe_info.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6cd5ea-8103-4572-9f1f-d2c8572ef5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle_logo(ID):\n",
    "    os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "    pickle_name = f'{ID}_video_intelligence.pickle'\n",
    "    objects = []\n",
    "\n",
    "    with (open(pickle_name, \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "    pickle_dict = objects[0]\n",
    "    logo_df = {'Entity ID': [], 'Description': [], 'Start Time': [], 'End Time': [], 'Confidence': [], 'Left': [], 'Top': [], 'Right': [], 'Bottom': []}\n",
    "    \n",
    "    \n",
    "    # LOGO detection\n",
    "    for logo_recognition_annotation in pickle_dict[\"logo detection\"]:\n",
    "        entity = logo_recognition_annotation.entity\n",
    "        \n",
    "        # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\n",
    "        # Search API](https://developers.google.com/knowledge-graph/).\n",
    "        \n",
    "\n",
    "        # All logo tracks where the recognized logo appears. Each track corresponds\n",
    "        # to one logo instance appearing in consecutive frames.\n",
    "        for track in logo_recognition_annotation.tracks:\n",
    "            logo_df[\"Entity ID\"].append(entity.entity_id)\n",
    "            logo_df[\"Description\"].append(entity.description)\n",
    "            logo_df[\"Start Time\"].append(track.segment.start_time_offset.seconds + track.segment.start_time_offset.microseconds * 1000)\n",
    "            logo_df[\"End Time\"].append(track.segment.end_time_offset.seconds + track.segment.end_time_offset.microseconds * 1000)\n",
    "            logo_df[\"Confidence\"].append(track.confidence)\n",
    "            \n",
    "            timestamped_object = track.timestamped_objects[0]\n",
    "            normalized_bounding_box = timestamped_object.normalized_bounding_box\n",
    "            \n",
    "            logo_df[\"Left\"].append(normalized_bounding_box.left)\n",
    "            logo_df[\"Top\"].append(normalized_bounding_box.top)\n",
    "            logo_df[\"Right\"].append(normalized_bounding_box.right)\n",
    "            logo_df[\"Bottom\"].append(normalized_bounding_box.bottom)\n",
    "\n",
    "    newdf = pd.DataFrame(data=logo_df) \n",
    "    os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "    newdf.to_csv(f\"{ID}_logo_info.csv\")\n",
    "    print('done with saving logo info')\n",
    "    \n",
    "def read_pickle_text(ID):\n",
    "    os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "    pickle_name = f'{ID}_video_intelligence.pickle'\n",
    "    objects = []\n",
    "\n",
    "    with (open(pickle_name, \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    pickle_dict = objects[0]\n",
    "    \n",
    "    text_df = {'Text': [], 'Start Time': [], 'End Time': [], 'Confidence': [], 'Time Offset': [], 'Vertex 1': [], 'Vertex 2': [], 'Vertex 3': [], 'Vertex 4': []}\n",
    "    \n",
    "    for text_annotation in pickle_dict[\"text detection\"]:\n",
    "        \n",
    "        text_df['Text'].append(text_annotation.text)\n",
    "\n",
    "        # Get the first text segment\n",
    "        text_segment = text_annotation.segments[0]\n",
    "        start_time = text_segment.segment.start_time_offset\n",
    "        end_time = text_segment.segment.end_time_offset\n",
    "        \n",
    "        text_df['Start Time'].append(start_time.seconds + start_time.microseconds * 1e-6)\n",
    "        text_df['End Time'].append(end_time.seconds + end_time.microseconds * 1e-6)\n",
    "        text_df['Confidence'].append(text_segment.confidence)\n",
    "\n",
    "        # Show the result for the first frame in this segment.\n",
    "        frame = text_segment.frames[0]\n",
    "        time_offset = frame.time_offset\n",
    "        text_df['Time Offset'].append(time_offset.seconds + time_offset.microseconds * 1e-6)\n",
    "\n",
    "        for i, vertex in enumerate(frame.rotated_bounding_box.vertices):\n",
    "            index = i+1\n",
    "            text_df[f'Vertex {index}'].append((vertex.x, vertex.y))\n",
    "        \n",
    "    newdf = pd.DataFrame(data=text_df)\n",
    "    os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "    newdf.to_csv(f\"{ID}_text_info.csv\")\n",
    "    print('done with saving text info')\n",
    "    \n",
    "def read_pickle_face(ID):\n",
    "    os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "    pickle_name = f'{ID}_video_intelligence.pickle'\n",
    "    objects = []\n",
    "\n",
    "    with (open(pickle_name, \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    pickle_dict = objects[0]\n",
    "    face_df = {'Start Time': [], 'End Time': [], 'Left': [], 'Top': [], 'Right': [], 'Bottom': [], 'glasses': [], 'headwear': [], 'eyes_visible': [], 'mouth_open': [], 'looking_at_camera': [], 'smiling': []}\n",
    "    \n",
    "    for annotation in pickle_dict[\"facial detection\"]:\n",
    "        # print(annotation)\n",
    "        for track in annotation.tracks:\n",
    "            face_df[\"Start Time\"].append(track.segment.start_time_offset.seconds + track.segment.start_time_offset.microseconds / 1e6)\n",
    "            face_df[\"End Time\"].append(track.segment.end_time_offset.seconds + track.segment.end_time_offset.microseconds / 1e6)\n",
    "            \n",
    "            \n",
    "            timestamped_object = track.timestamped_objects[0]\n",
    "            box = timestamped_object.normalized_bounding_box\n",
    "            \n",
    "            face_df[\"Left\"].append(box.left)\n",
    "            face_df[\"Top\"].append(box.top)\n",
    "            face_df[\"Right\"].append(box.right)\n",
    "            face_df[\"Bottom\"].append(box.bottom)\n",
    "            \n",
    "            # print(timestamped_object.attributes)\n",
    "            for attribute in timestamped_object.attributes:\n",
    "                # print(attribute)\n",
    "                face_df[str(attribute.name)].append(attribute.confidence)\n",
    "    \n",
    "    if len(face_df[\"Start Time\"]) != len(face_df[\"glasses\"]):\n",
    "        difference = len(face_df[\"Start Time\"]) - len(face_df[\"glasses\"])\n",
    "        # print(face_df.items()[0])\n",
    "        for column, listt in list(face_df.items())[6:]:\n",
    "            listt.extend([None]*difference)\n",
    "            print(column, len(listt))\n",
    "        \n",
    "    newdf = pd.DataFrame(data=face_df)\n",
    "    os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "    newdf.to_csv(f\"{ID}_faces_info.csv\", index=True)\n",
    "    print('done with saving face info', ID)\n",
    "    \n",
    "def read_pickle_label(ID):\n",
    "    os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "    pickle_name = f'{ID}_video_intelligence.pickle'\n",
    "    objects = []\n",
    "\n",
    "    with (open(pickle_name, \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    pickle_dict = objects[0]\n",
    "    \n",
    "    video_df = {'Description': [], 'Label Category Description': [], 'Time Start': [], 'Time End': [], 'Confidence': []}\n",
    "    shot_df = {'Description': [], 'Label Category Description': [], 'Time Start': [], 'Time End': [], 'Confidence': []}\n",
    "    frame_df = {'Description': [], 'Label Category Description': [], 'Frame Time Offset': [], 'Confidence': []}\n",
    "    \n",
    "    for segment_label in pickle_dict[\"segment labels\"]:\n",
    "        category = ''\n",
    "        if len(segment_label.category_entities) != 0:\n",
    "            category = segment_label.category_entities[0].description\n",
    "        for segment in segment_label.segments:\n",
    "            video_df['Description'].append(segment_label.entity.description)\n",
    "            video_df['Label Category Description'].append(category)\n",
    "            video_df['Time Start'].append(segment.segment.start_time_offset.seconds + segment.segment.start_time_offset.microseconds / 1e6)\n",
    "            video_df['Time End'].append(segment.segment.end_time_offset.seconds + segment.segment.end_time_offset.microseconds / 1e6)\n",
    "            video_df['Confidence'].append(segment.confidence)\n",
    "                \n",
    "    for shot_label in pickle_dict[\"shot labels\"]:\n",
    "        category = ''\n",
    "        if len(shot_label.category_entities) != 0:\n",
    "            category = shot_label.category_entities[0].description\n",
    "        for shot in shot_label.segments:\n",
    "            shot_df['Description'].append(shot_label.entity.description)\n",
    "            shot_df['Label Category Description'].append(category)\n",
    "            shot_df['Time Start'].append(shot.segment.start_time_offset.seconds + shot.segment.start_time_offset.microseconds / 1e6)\n",
    "            shot_df['Time End'].append(shot.segment.end_time_offset.seconds + shot.segment.end_time_offset.microseconds / 1e6)\n",
    "            shot_df['Confidence'].append(shot.confidence)\n",
    "        \n",
    "    for frame_label in pickle_dict[\"frame labels\"]:\n",
    "        category = ''\n",
    "        if len(frame_label.category_entities) != 0:\n",
    "            category = frame_label.category_entities[0].description\n",
    "        for frame in frame_label.frames:\n",
    "            shot_df['Description'].append(frame_label.entity.description)\n",
    "            shot_df['Label Category Description'].append(category)\n",
    "            shot_df['Frame Time Offset'].append(frametime_offset.seconds + frame.time_offset.microseconds / 1e6)\n",
    "            shot_df['Confidence'].append(frame.confidence)\n",
    "                \n",
    "    videodf = pd.DataFrame(data=video_df)\n",
    "    shotdf = pd.DataFrame(data=shot_df)\n",
    "    framedf = pd.DataFrame(data=frame_df)\n",
    "    os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "    videodf.to_csv(f\"{ID}_labelvideo_info.csv\", index=True)\n",
    "    shotdf.to_csv(f\"{ID}_labelshot_info.csv\", index=True)\n",
    "    framedf.to_csv(f\"{ID}_labelframe_info.csv\", index=True)\n",
    "    \n",
    "    print('done with saving label info', ID)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2ec1d-6d99-4f54-ae27-b6af6b6c166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 'sb_2023_3515'\n",
    "os.chdir(f\"{PARENT_DIR}/{ID}\")\n",
    "# detect_person(ID+'.mp4')\n",
    "read_pickle_label(ID)\n",
    "# read_pickle_text(ID)\n",
    "# read_pickle_logo(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae8bb0-b571-4c03-b229-738a23c5f36a",
   "metadata": {},
   "source": [
    "# Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47137417-3f58-4b57-b496-9f85a757d27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video for logo annotations: sb_2002_1426\n",
      "\n",
      "Finished processing.\n",
      "\n",
      "Processing video for face detection annotations.\n",
      "\n",
      "Finished processing.\n",
      "\n",
      "done saving pickle\n",
      "done with saving face info sb_2002_1426\n",
      "done with saving text info\n",
      "done with saving logo info\n",
      "done with saving label info sb_2002_1426\n"
     ]
    }
   ],
   "source": [
    "CSV_NAME = \"nw_may_v1.csv\"\n",
    "os.chdir(f\"{LINK_DIR}\")\n",
    "df = pd.read_csv(CSV_NAME)\n",
    "\n",
    "def run_all(df):\n",
    "    for i in range(1500, df.shape[0]):\n",
    "        \n",
    "        if df[\"has_vid\"][i] == 0:\n",
    "            continue\n",
    "        \n",
    "        ID = df['id'][i]\n",
    "        \n",
    "        save_to_pickle(ID)\n",
    "        read_pickle_face(ID)\n",
    "        read_pickle_text(ID)\n",
    "        read_pickle_logo(ID)\n",
    "        read_pickle_label(ID)\n",
    "\n",
    "def run_one(ID):\n",
    "    save_to_pickle(ID)\n",
    "    read_pickle_face(ID)\n",
    "    read_pickle_text(ID)\n",
    "    read_pickle_logo(ID)\n",
    "    read_pickle_label(ID)\n",
    "    \n",
    "run_one('sb_2002_1426')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a55bd9-4795-43f5-abf3-36d88666a60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c7cc3-d9e6-4235-882a-08f5835a103c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
