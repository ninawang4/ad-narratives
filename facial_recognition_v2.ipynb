{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523cab2-599e-4581-8646-5d18d8326727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import ffmpeg\n",
    "import moviepy.editor as mp\n",
    "import cv2\n",
    "import io\n",
    "import PIL\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "import base64\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0714f-1637-4b60-9755-512949732f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/nfs/sloanlab003/projects/arc_proj/story/data/sb_ad_links\")\n",
    "LINK_DIR = \"/nfs/sloanlab003/projects/arc_proj/story/data/sb_ad_links\"\n",
    "PARENT_DIR = \"/nfs/sloanlab003/projects/arc_proj/story/data/sb_ad_vids\"\n",
    "df = pd.read_csv(\"nw_april_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a597fb-8c77-4dbd-9d41-789daf4715a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import videointelligence_v1 as videointelligence\n",
    "from google.cloud import vision\n",
    "\n",
    "os.chdir(\"/nfs/sloanlab003/projects/arc_proj/story/code/nw/google-cloud-sdk\")\n",
    "GOOGLE_APPLICATION_CREDENTIALS = \"/nfs/sloanlab003/projects/arc_proj/story/code/nw/ad-story-arc-b60c485322b4.json\"\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d3173-33ed-49a7-bcc6-e3f3fb6ddfc1",
   "metadata": {},
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df['id'] = [None]\n",
    "new_df['video_id'] = [None]\n",
    "new_df['bounding_box'] = [None]\n",
    "new_df['emotions'] = [None]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de11b323-2c21-4396-a714-42cf23db3ade",
   "metadata": {},
   "source": [
    "new_df = pd.DataFrame()\n",
    "temp_df = pd.DataFrame({\n",
    "    \"id\" : \"nope\",\n",
    "    \"video_id\": \"slayage\",\n",
    "    \"bounding_box\": \"ur mom\",\n",
    "    \"emotions\" : \"stop\"\n",
    "}, index=[len(new_df)])\n",
    "#\n",
    "# Append a dataframe\n",
    "#\n",
    "new_df = new_df.append(temp_df)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145eb27-829b-4d29-88da-5c36cb12ff25",
   "metadata": {},
   "source": [
    "## Google Vision API Face Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36186eb-3195-4561-8b9e-2710e3f1b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import PIL\n",
    "import math\n",
    "import shutil\n",
    "import base64\n",
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "os.chdir(\"/nfs/sloanlab003/projects/arc_proj/story/data/sb_ad_links\")\n",
    "PARENT_DIR = \"/nfs/sloanlab003/projects/arc_proj/story/data/sb_ad_vids\"\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "# feed video into Google vision API to see what \n",
    "for i in range(3498, df.shape[0]):\n",
    "    \n",
    "    if df['has_vid'][i] == 0:\n",
    "        continue\n",
    "    \n",
    "    # store video id and length\n",
    "    ID = df['id'][i]\n",
    "    \n",
    "    # cd into video's directory\n",
    "    os.chdir(f'{PARENT_DIR}/{ID}/img_frames')\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f'starting to parse {ID}')\n",
    "    \n",
    "    if i % 100 == 0 and i != 3500:\n",
    "        os.chdir(LINK_DIR)\n",
    "        new_df.to_csv(f\"nw_faceinfo_{i-99}-{i}.csv\")\n",
    "        print(f\"saved nw_faceinfo_{i-99}-{i}.csv\")\n",
    "        new_df = pd.DataFrame()\n",
    "        os.chdir(f'{PARENT_DIR}/{ID}/img_frames')\n",
    "        \n",
    "    for filename in os.listdir(f'{PARENT_DIR}/{ID}/img_frames'):\n",
    "\n",
    "        frame = filename\n",
    "        frame_id = frame[:-4]\n",
    "        \n",
    "        im = PIL.Image.open(frame, 'r')\n",
    "        bounding_boxes = google_image_face(PARENT_DIR, ID, frame)\n",
    "        \n",
    "        # for each face detected, save the bounding box as seperate image\n",
    "        for j, face_info in enumerate(bounding_boxes):\n",
    "            \n",
    "            index = str(j+1)\n",
    "            x = index.zfill(2)\n",
    "            \n",
    "            bounding_box = str(face_info[0])\n",
    "            emotions = face_info[1]\n",
    "\n",
    "            temp_df = pd.DataFrame({\n",
    "                \"id\" : f\"{frame_id}_f{x}\",\n",
    "                \"video id\": ID,\n",
    "                \"bounding box\": bounding_box,\n",
    "                \"emotions\" : emotions\n",
    "            }, index=[len(new_df)])\n",
    "            \n",
    "            try:\n",
    "                new_df = new_df.append(temp_df)\n",
    "                \n",
    "            except:\n",
    "                print('something went wrong')\n",
    "            \n",
    "            \n",
    "            \n",
    "print('finished')\n",
    "os.chdir(LINK_DIR)\n",
    "new_df.to_csv(\"nw_faceinfo_v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead6f7a-271a-49ca-b1a7-3e1fc8db6c22",
   "metadata": {},
   "source": [
    "## Cloud vision API - Function Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df462c94-2a9c-4a5b-b2e7-df8b75407017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_image_face(parent_dir, id_, path):\n",
    "    \n",
    "    os.chdir(f'{parent_dir}/{id_}/img_frames')\n",
    "    from google.oauth2 import service_account\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(\"/nfs/sloanlab003/projects/arc_proj/story/code/nw/ad-story-arc-b60c485322b4.json\")\n",
    "    client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.face_detection(image=image)\n",
    "    faces = response.face_annotations\n",
    "    \n",
    "\n",
    "    # Names of likelihood from google.cloud.vision.enums\n",
    "    likelihood_name = ('UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE',\n",
    "                       'LIKELY', 'VERY_LIKELY')\n",
    "    \n",
    "    face_info = []\n",
    "    \n",
    "    for face in faces:\n",
    "        face_labels = ['anger: {}'.format(likelihood_name[face.anger_likelihood]), 'joy: {}'.format(likelihood_name[face.joy_likelihood]), 'surprise: {}'.format(likelihood_name[face.surprise_likelihood]), \n",
    "                  'sorrow: {}'.format(likelihood_name[face.sorrow_likelihood]), 'under_exposed: {}'.format(likelihood_name[face.under_exposed_likelihood]), \n",
    "                  'blurred: {}'.format(likelihood_name[face.blurred_likelihood]), 'headwear: {}'.format(likelihood_name[face.headwear_likelihood])]\n",
    "        \n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                    for vertex in face.bounding_poly.vertices])\n",
    "        \n",
    "        bounding_box = crop(face)       \n",
    "        face_info.append([bounding_box, '\\n'.join(face_labels)])\n",
    "        \n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))\n",
    "        \n",
    "    return face_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66ccfd-1af2-4783-8b3a-09863131df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP_MARGIN_PERCENT = 10\n",
    "\n",
    "def crop(face_annotation):\n",
    "    v = face_annotation.bounding_poly.vertices\n",
    "    x1, y1, x2, y2 = v[0].x, v[0].y, v[2].x + 1, v[2].y + 1\n",
    "    w, h = x2 - x1, y2 - y1\n",
    "    hx, hy = x1 + w / 2, y1 + h / 2\n",
    "    m = max(w, h) * (100 + CROP_MARGIN_PERCENT) / 100 / 2\n",
    "    return int(hx - m + 0.5), int(hy - m + 0.5), int(hx + m + 1.5), int(hy + m + 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e2810-70e6-41c1-b2e4-8a6a722ddc26",
   "metadata": {},
   "source": [
    "## Cloud Video Intelligence API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fad44-466e-4323-be93-bc3127b36b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_person(local_file_path):\n",
    "    \"\"\"Detects people in a video from a local file.\"\"\"\n",
    "    from google.oauth2 import service_account\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(\"/nfs/sloanlab003/projects/arc_proj/story/code/nw/ad-story-arc-b60c485322b4.json\")\n",
    "    \n",
    "    client = videointelligence.VideoIntelligenceServiceClient(credentials=credentials)\n",
    "\n",
    "    with io.open(local_file_path, \"rb\") as f:\n",
    "        input_content = f.read()\n",
    "\n",
    "    # Configure the request\n",
    "    config = videointelligence.FaceDetectionConfig(\n",
    "        include_bounding_boxes=True, include_attributes=True\n",
    "    )\n",
    "    context = videointelligence.VideoContext(face_detection_config=config)\n",
    "\n",
    "    # Start the asynchronous request\n",
    "    operation = client.annotate_video(\n",
    "        request={\n",
    "            \"features\": [videointelligence.Feature.FACE_DETECTION],\n",
    "            \"input_content\": input_content,\n",
    "            \"video_context\": context,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\nProcessing video for face detection annotations.\")\n",
    "    result = operation.result(timeout=300)\n",
    "\n",
    "    print(\"\\nFinished processing.\\n\")\n",
    "\n",
    "    # Retrieve the first result, because a single video was processed.\n",
    "    annotation_result = result.annotation_results[0]\n",
    "    \n",
    "    return_list = []\n",
    "\n",
    "    for annotation in annotation_result.face_detection_annotations:\n",
    "        face_info = []\n",
    "        \n",
    "        # print(\"Face detected:\")\n",
    "        for track in annotation.tracks:\n",
    "\n",
    "            timestamped_object = track.timestamped_objects[0]\n",
    "            box = timestamped_object.normalized_bounding_box\n",
    "            \n",
    "            \n",
    "            face_info.append({\"box\": (box.left, box.top, box.right, box.bottom)})\n",
    "            face_info.append({\"start_time\": track.segment.start_time_offset.seconds + track.segment.start_time_offset.microseconds / 1e6})\n",
    "            face_info.append({\"end_time\": track.segment.end_time_offset.seconds + track.segment.end_time_offset.microseconds / 1e6})\n",
    "\n",
    "\n",
    "\n",
    "            return_list.append(face_info)\n",
    "    # print('here', return_list)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab60a9-66fc-4fda-aef8-5076deff36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_from_video(ID, bounding_box, timestamp):\n",
    "    os.chdir(f'{PARENT_DIR}/{ID}')\n",
    "    filename = ID + '.mp4'\n",
    "    \n",
    "    vidcap = cv2.VideoCapture(filename)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    success = True\n",
    "\n",
    "    while success:\n",
    "        success,frame = vidcap.read()\n",
    "\n",
    "        count+=1\n",
    "        if count/fps >= timestamp:\n",
    "            # print('frame #', count, 'time', count/fps)\n",
    "            imageRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            im = Image.fromarray(imageRGB)\n",
    "            roi = crop_normalized(bounding_box, im)\n",
    "            \n",
    "            vidcap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            return roi\n",
    "            break\n",
    "            \n",
    "    vidcap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad8074-f102-4397-b6c2-6c02264c2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_normalized(bounding_box, image):\n",
    "    left, top, right, bottom = bounding_box[0], bounding_box[1], bounding_box[2], bounding_box[3]\n",
    "    number_of_rows, number_of_columns = image.height, image.width\n",
    "    \n",
    "    top_unormalized = round(top * number_of_rows)\n",
    "    left_unormalized = round(left * number_of_columns)\n",
    "    bottom_unormalized = round(bottom * number_of_rows)\n",
    "    right_unormalized = round(right * number_of_columns)\n",
    "    \n",
    "    return image.crop((left_unormalized, top_unormalized, right_unormalized, bottom_unormalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72301b51-4d2b-4641-8204-39d1d9366152",
   "metadata": {},
   "source": [
    "## Running videos through Video Intelligence API\n",
    "Saves faces as cropped frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5e906-38ac-444a-b97b-00eb31ebb5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run videos through video intelligence API\n",
    "os.chdir(LINK_DIR)\n",
    "df = pd.read_csv('temp_1500-2000.csv')\n",
    "\n",
    "num_faces_save = 10\n",
    "df2 = df.copy()\n",
    "\n",
    "df_num = 1\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    \n",
    "    if df[\"has_vid\"][i] == 0:\n",
    "        continue\n",
    "    \n",
    "    ID = df[\"id\"][i]\n",
    "    os.chdir(f'{PARENT_DIR}/{ID}')\n",
    "    \n",
    "    try:\n",
    "        result = detect_person(ID+\".mp4\")\n",
    "        segment_length = {}\n",
    "        os.mkdir(f\"{PARENT_DIR}/{ID}/google_video_face_samples\")\n",
    "    \n",
    "        for j, face in enumerate(result[::-1]):\n",
    "            bb = face[0]['box']\n",
    "            start_time = face[1]['start_time']\n",
    "            end_time = face[2]['end_time']\n",
    "            segment_length[end_time - start_time] = (bb, start_time)\n",
    "\n",
    "        num_faces = len(segment_length.keys())\n",
    "        top_faces = sorted(segment_length.keys(), reverse=True)[:num_faces_save]\n",
    "\n",
    "        df2['faces detected'][i] = num_faces\n",
    "\n",
    "        if i % 500 == 0 and i != 0:\n",
    "            os.chdir(LINK_DIR)\n",
    "            df2.to_csv(f'temp_{i-500}-{i}.csv', index=False)\n",
    "\n",
    "\n",
    "        for j, segment in enumerate(top_faces):\n",
    "            (bb, start_time) = segment_length[segment]\n",
    "            roi = crop_from_video(ID, bb, start_time)\n",
    "            roi.save(f\"{PARENT_DIR}/{ID}/google_video_face_samples/{str(j).zfill(3)}_{str(round(start_time, 2)).zfill(5)}.jpg\")\n",
    "\n",
    "        print('done with', ID)\n",
    "    except:\n",
    "        didnt_work.append(ID)\n",
    "        print(ID, \"didn't work\")\n",
    "        \n",
    "    \n",
    "os.chdir(LINK_DIR)\n",
    "df2.to_csv('TEST.csv', index=False)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce03c2-652d-4c46-a06a-b6c5822fccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 10 sample videos through google vision API\n",
    "os.chdir(f\"{LINK_DIR}\")\n",
    "df = pd.read_csv(\"nw_april_faceinfo_ALL.csv\")\n",
    "for ID in id_test:\n",
    "    os.chdir(f'{PARENT_DIR}/{ID}/img_frames')\n",
    "    counter = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        os.chdir(f'{PARENT_DIR}/{ID}/img_frames')\n",
    "        if df[\"video id\"][i] != ID:\n",
    "            continue\n",
    "        \n",
    "        frame_path = df[\"id\"][i][:-4]+\".jpg\"\n",
    "        print(frame_path)\n",
    "        im = PIL.Image.open(frame_path, 'r')\n",
    "        bounding_box = tuple(map(int, df[\"bounding box\"][i][1:-1].split(', ')))\n",
    "        roi = im.crop(bounding_box)\n",
    "        \n",
    "        os.chdir(f\"{LINK_DIR}/google_vision_face_samples\")\n",
    "        roi.save(f\"{frame_path}_face_{counter}.jpg\")\n",
    "        counter += 1\n",
    "        \n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c286a8-ac25-4593-8963-cc6b03bb7c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352bdb7-73ce-457c-8559-e3996418aeb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bd16f-0240-4af1-afc3-043373415432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a15bbb-7719-46c0-97a1-d2472744f5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
